import streamlit as st
import requests
from bs4 import BeautifulSoup
import urllib.parse

# 1. ç½‘é¡µæ ‡é¢˜å’Œè¯´æ˜ (ä½ å¯ä»¥åœ¨è¿™é‡Œéšæ„ä¿®æ”¹æ–‡å­—)
st.set_page_config(page_title="AI é¹°çœ¼ - ç½‘ç«™ AI å‹å¥½åº¦ä½“æ£€", page_icon="ğŸ¦…")
st.title("ğŸ¦… AI é¹°çœ¼ï¼šä½ çš„ç½‘ç«™èƒ½è¢«å¤§æ¨¡å‹æœåˆ°å—ï¼Ÿ")
st.markdown("è¾“å…¥ä½ çš„ç½‘å€ï¼Œä¸€é”®æ£€æµ‹ç½‘ç«™çš„ **AI å‹å¥½åº¦ (GEO)** è¯„åˆ†ï¼")

# 2. è¾“å…¥æ¡†å’ŒæŒ‰é’®
url_input = st.text_input("ğŸ”— è¯·è¾“å…¥è¦æµ‹è¯•çš„ç½‘å€ (éœ€åŒ…å« http:// æˆ– https://):", "https://www.apple.com.cn/")
submit_button = st.button("ğŸš€ ç«‹å³å…è´¹ä½“æ£€")

# 3. å½“ç”¨æˆ·ç‚¹å‡»æŒ‰é’®åå‘ç”Ÿçš„äº‹æƒ…
if submit_button:
    if not url_input.startswith("http"):
        st.error("âŒ ç½‘å€æ ¼å¼ä¸å¯¹å“¦ï¼Œè¯·åŠ ä¸Š http:// æˆ– https://")
    else:
        with st.spinner('ğŸ•¸ï¸ æ­£åœ¨æ¨¡æ‹Ÿ AI çˆ¬è™«æ‰«æä¸­ï¼Œè¯·ç¨å€™...'):
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
            try:
                # è·å–ç½‘é¡µå†…å®¹
                response = requests.get(url_input, headers=headers, timeout=10)
                html_content = response.text
                soup = BeautifulSoup(html_content, 'html.parser')
                
                st.subheader("ğŸ“Š ä½ çš„ä¸“å±ä½“æ£€æŠ¥å‘Š")
                
                # --- æŒ‡æ ‡ä¸€ï¼šéª¨æ¶æ¸…æ™°åº¦ ---
                st.markdown("### ğŸ” æŒ‡æ ‡ä¸€ï¼šç½‘é¡µéª¨æ¶æ¸…æ™°åº¦ (H1/H2æ ‡ç­¾)")
                h1_tags = soup.find_all('h1')
                h2_tags = soup.find_all('h2')
                
                # ç”¨æ¼‚äº®çš„æŒ‡æ ‡å¡ç‰‡æ˜¾ç¤ºæ•°å­—
                col1, col2 = st.columns(2)
                col1.metric("H1 ä¸»æ ‡é¢˜æ•°é‡", len(h1_tags))
                col2.metric("H2 å‰¯æ ‡é¢˜æ•°é‡", len(h2_tags))
                
                if h1_tags:
                    st.success("âœ… ä¼˜ç§€ï¼æ‰¾åˆ°äº† <h1> æ ‡ç­¾ï¼ŒAI èƒ½ç§’æ‡‚ä½ çš„æ ¸å¿ƒä¸»é¢˜ã€‚")
                else:
                    st.error("âŒ ä¸¥é‡è­¦å‘Šï¼šç¼ºå¤± <h1> æ ‡ç­¾ï¼AI æŠ“å–æ—¶ä¼šæ‰¾ä¸åˆ°é‡ç‚¹ã€‚")

                # --- æŒ‡æ ‡äºŒï¼šçº¯æ–‡æœ¬å«é‡‘é‡ ---
                st.markdown("### ğŸ” æŒ‡æ ‡äºŒï¼šçº¯æ–‡æœ¬å«é‡‘é‡ (ä¿¡å™ªæ¯”)")
                for script in soup(["script", "style"]):
                    script.extract() # æ¸…é™¤æ— ç”¨ä»£ç 
                
                pure_text = soup.get_text(strip=True)
                ratio = (len(pure_text) / len(html_content)) * 100 if len(html_content) > 0 else 0
                
                st.metric("ä¿¡å™ªæ¯”å¾—åˆ† (å»ºè®®å¤§äº10%)", f"{ratio:.2f}%")
                
                if ratio < 10:
                    st.warning("âš ï¸ ä»£ç å¤ªè‡ƒè‚¿ï¼çœŸå®å†…å®¹è¢«å¤§é‡æ— æ•ˆä»£ç æ·¹æ²¡ï¼ŒAI æå–å›°éš¾ã€‚")
                else:
                    st.success("âœ… ä¿¡å™ªæ¯”å¥åº·ï¼ŒAI æå–æ­£æ–‡éå¸¸è½»æ¾ã€‚")

                # --- æŒ‡æ ‡ä¸‰ï¼šçˆ¬è™«æ‹¦æˆªæµ‹è¯• ---
                st.markdown("### ğŸ” æŒ‡æ ‡ä¸‰ï¼šAI çˆ¬è™«å¤§é—¨æµ‹è¯• (robots.txt)")
                parsed_url = urllib.parse.urlparse(url_input)
                robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
                robots_response = requests.get(robots_url, headers=headers, timeout=5)
                
                if robots_response.status_code == 200:
                    st.info("â„¹ï¸ ç½‘ç«™é…ç½®äº† robots.txtï¼Œå»ºè®®äººå·¥ç¡®è®¤æ˜¯å¦æ‹¦æˆªäº† AI çˆ¬è™«ã€‚")
                else:
                    st.success("âœ… æœªæ£€æµ‹åˆ°ä¸¥æ ¼çš„ robots.txt æ‹¦æˆªï¼ŒAI é»˜è®¤å¯è®¿é—®ã€‚")
                    
                st.balloons() # åº†ç¥åŠ¨ç”» ğŸ‰
                
            except Exception as e:
                st.error(f"âŒ æ‰«æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘å€æ˜¯å¦æ­£ç¡®æˆ–ç½‘ç«™å¼€å¯äº†é˜²æŠ“å–ä¿æŠ¤ã€‚")
